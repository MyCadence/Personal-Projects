# hw4.py

import os.path
import numpy
import matplotlib.pyplot as plt

######################################
#
# FUNCTIONS YOU WILL NEED TO MODIFY:
#  - linreg_closed_form
#  - loss
#  - linreg_grad_desc
#  - random_fourier_features
#
######################################

def linreg_model_sample(Theta,model_X):
	if model_X.shape[1]==1:
		## get a bunch of evenly spaced X values in the same range as the passed in data
		sampled_X = numpy.linspace(model_X.min(axis=0),model_X.max(axis=0),100)
		## get the Y values for our sampled X values by taking the dot-product with the model
		## Note: we're appending a column of all ones so we can do this with a single matrix-vector multiply
		sampled_Y = numpy.hstack([numpy.ones((sampled_X.shape[0],1)),sampled_X]).dot(Theta)
		return sampled_X, sampled_Y
	elif model_X.shape[1]==2:
		## Unfortunately, plotting surfaces is a bit more complicated, first we need
		## a set of points that covers the area we want to plot. numpy.meshgrid is a helper function
		## that will create two NxN arrays that vary over both the X and Y range given.
		sampled_X, sampled_Y = numpy.meshgrid(model_X[:,0],model_X[:,1])
		## We can't just do a simple matrix multiply here, because plot_surface(...) is going to expect NxN arrays like
		## those generated by numpy.meshgrid(...). So here we're explicitly pulling out the components of Theta as
		## scalars and multiplying them across each element in the X and Y arrays to get the value for Z
		sampled_Z = sampled_X*Theta[1]+sampled_Y*Theta[2]+Theta[0]
		return sampled_X, sampled_Y, sampled_Z

def plot_helper(data_X, data_Y, model_X=None, model_Y=None, model_Z=None):
	## 2D plotting
	## data_X.shape[1] is the number of columns in data_X, just as data_X.shape[0] is the number of rows
	if data_X.shape[1]==1: 
		fig1 = plt.figure() ## creates a new figure object that we can plot into
		fig1.gca().scatter(data_X,data_Y) ## creates a scatterplot with the given set of X and Y points
		## If we were given a model, we need to plot that
		if not(model_X is None) and not(model_Y is None):
			## Plot the data from the model
			## Note: we're using plot(...) instead of scatter(...) because we want a smooth curve
			fig1.gca().plot(model_X,model_Y,color='r')
		## The graph won't actually be displayed until we .show(...) it. You can swap this with savefig(...) if you
		## instead want to save an image of the graph instead of displaying it. You can also use the interface to save an
		## image after displaying it
		plt.show() #fig1.show()
	## 3D plotting
	elif data_X.shape[1]==2:
		## This import statement 'registers' the ability to do 3D projections/plotting with matplotlib
		from mpl_toolkits.mplot3d import Axes3D
		fig1 = plt.figure()
		## The format for 3D scatter is similar to 2D; just add the third dimension to the argument list
		fig1a = fig1.add_subplot(projection='3d')
		fig1a.scatter(data_X[:,0], data_X[:,1],data_Y)
		## Older versions of matplotlib allowed keyword arguments in the call to gca(), so the above 
		## two lines could be done in one. If you are using an older version of matplotlib that 
		## fails on the above lines, try line commented below, and the commented line inside the 
		## following if block
		# fig1.gca(projection='3d').scatter(data_X[:,0],data_X[:,1],data_Y)
		if not(model_X is None) and not(model_Y is None) and not(model_Z is None):
			## Now, with our X, Y, and Z arrays (all NxN), we can use plot_surface(...) to create a nice 3D surface
			fig1a.plot_surface(model_X, model_Y, model_Z,linewidth=0.0,color=(1.0,0.2,0.2,0.75))
			## See note in comment above
			# fig1.gca(projection='3d').plot_surface(model_X, model_Y, model_Z,linewidth=0.0,color=(1.0,0.2,0.2,0.75))
		plt.show() #fig1.show()
	else:
		## Matplotlib does not yet have the capability to plot in 4D
		print('Data is not in 2 or 3 dimensions, cowardly refusing to plot! (data_X.shape == {})'.format(data_X.shape))

## Data loading utility function
def load_data(fname,directory='data'):
	data = numpy.loadtxt(os.path.join(directory,fname),delimiter=',')
	rows,cols = data.shape
	X_dim = cols-1
	Y_dim = 1
	return data[:,:-1].reshape(-1,X_dim), data[:,-1].reshape(-1,Y_dim)

def vis_linreg_model(train_X, train_Y, Theta):
    # Get model sample based on the number of features in train_X
    if train_X.shape[1] == 1:  # 1D case
        sample_X, sample_Y = linreg_model_sample(Theta, train_X)
        plot_helper(train_X, train_Y, sample_X, sample_Y)
    elif train_X.shape[1] == 2:  # 2D (noisy) case
        sample_X, sample_Y, sample_Z = linreg_model_sample(Theta, train_X)
        plot_helper(train_X, train_Y, sample_X, sample_Y, sample_Z)
    else:
        print("Error: Only 1D or 2D data supported for linear regression plot.")

###################
# YOUR CODE BELOW #
###################
def linreg_closed_form(train_X, train_Y):
	'''
	Computes the optimal parameters for the given training data in closed form


	Args:
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector


	Returns:
		A length D+1 numpy array with the optimal parameters	
	'''
	# Prepend a column of ones to train_X for the intercept term
	X = numpy.hstack([numpy.ones((train_X.shape[0], 1)), train_X])
	# Compute closed-form solution
	Theta = numpy.linalg.inv(X.T @ X) @ (X.T @ train_Y)
	return Theta

###################
# YOUR CODE BELOW #
###################
def loss(Theta, train_X, train_Y):
	'''
	Computes the squared loss for the given setting of the parameters given the training data


	Args:
		Theta (length D+1 numpy array): the parameters of the model
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector


	Returns:
		The (scalar) loss for the given parameters and data.
	'''
	# Prepend a column of ones to train_X for the intercept term
	X = numpy.hstack([numpy.ones((train_X.shape[0], 1)), train_X])
	# Compute predictions
	predictions = X @ Theta
	# Compute loss
	rv = numpy.mean((predictions - train_Y) ** 2) / 2
	return rv

###################
# YOUR CODE BELOW #
###################
def linreg_grad_desc(initial_Theta, train_X, train_Y, alpha=0.05, num_iters=500, print_iters=True):
	'''
	Fits parameters using gradient descent


	Args:
		initial_Theta ((D+1)-by-1 numpy array): The initial value for the parameters we're optimizing over
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (N-by-1 numpy array): The training data target as a vector
		alpha (float): the learning rate/step size, defaults to 0.05
		num_iters (int): number of iterations to run gradient descent for, defaults to 500


	Returns:
		The history of theta's and their associated loss as a list of tuples [ (Theta1,loss1), (Theta2,loss2), ...]
	'''
    # Prepend a column of ones to train_X for the intercept term
	X = numpy.hstack([numpy.ones((train_X.shape[0], 1)), train_X])
	cur_Theta = initial_Theta
	step_history = []

	for k in range(1, num_iters + 1):
		# Compute gradient of the loss
		gradient = (X.T @ (X @ cur_Theta - train_Y)) / train_X.shape[0]
		# Update parameters
		cur_Theta -= alpha * gradient
		# Compute current loss
		cur_loss = numpy.mean((X @ cur_Theta - train_Y) ** 2) / 2
		step_history.append((cur_Theta.copy(), cur_loss))
		if print_iters:
			print(f"Iteration: {k}, Loss: {cur_loss:.5f}, Theta: {cur_Theta.flatten()}")

	return step_history

def apply_RFF_transform(X,Omega,B):
	'''
	Transforms features into a Fourier basis with given samples

		Given a set of random inner products and translations, transform X into the Fourier basis, Phi(X)
			phi_k(x) = cos(<x,omega_k> + b_k)                           #scalar form
			Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)].T  #vector form
			Phi(X) = [Phi(x_1), Phi(x_2), ..., Phi(x_N)].T              #matrix form


	Args:
		X (N-by-D numpy array): matrix of row-vector features (may also be a single row-vector)
		Omega (D-by-NFF numpy array): matrix of row-vector inner products
		B (NFF length numpy array): vector of translations



	Returns:
		A N-by-NFF numpy array matrix of transformed points, Phi(X)
	'''
	return numpy.sqrt(1.0/Omega.shape[1])*numpy.cos(X.dot(Omega)+B)

##################
# YOUR CODE HERE #
##################
def random_fourier_features(train_X, train_Y, num_fourier_features=100, alpha=0.05, num_iters=500, print_iters=False):
	'''
	Creates a random set of Fourier basis functions and fits a linear model in this space.

		Randomly sample num_fourier_features's non-linear transformations of the form:

			phi_k(x) = cos(<x,omega_k> + b_k)
			Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)]

		where omega_k and b_k are sampled according to (Rahimi and Recht, 20018). 


	Args:
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector
		num_fourier_features (int): the number of random features to generate


	Returns:
		Theta (numpy array of length num_fourier_features+1): the weights for the *transformed* model
		Omega (D-by-num_fourier_features numpy array): the inner product term of the transformation
		B (numpy array of length num_fourier_features): the translation term of the transformation
	'''
	# You will find the following functions useful for sampling:
	# 	numpy.random.multivariate_normal() for normal random variables
	#	numpy.random.random() for Uniform random variables
	D = train_X.shape[1]
	# Sample Omega from a Gaussian distribution
	Omega = numpy.random.multivariate_normal(mean=numpy.zeros(D), cov=numpy.eye(D), size=num_fourier_features).T
	# Sample B from a uniform distribution
	B = 2 * numpy.pi * numpy.random.random(num_fourier_features)
	# Transform training data into Fourier basis
	Phi = apply_RFF_transform(train_X, Omega, B)
	# Initialize Theta
	initial_Theta = numpy.zeros((Phi.shape[1] + 1, 1))
	# Perform gradient descent
	step_history = linreg_grad_desc(initial_Theta, Phi, train_Y, alpha=alpha, num_iters=num_iters, print_iters=print_iters)
	return step_history[-1][0], Omega, B

def rff_model_sample(Theta,Omega,B,model_X):
	sampled_X = numpy.linspace(model_X.min(axis=0),model_X.max(axis=0),100)
	Phi = apply_RFF_transform(sampled_X,Omega,B)
	sampled_Y = Phi.dot(Theta)
	return sampled_X, sampled_Y

def vis_rff_model(train_X, train_Y, Theta, Omega, B):
	sample_X, sample_Y = rff_model_sample(Theta,Omega,B, train_X)
	plot_helper(train_X, train_Y, sample_X, sample_Y)

if __name__ == '__main__':
    # Define values of K for small, medium, and large
    k_values = {'small': 10, 'medium': 50, 'large': 200}

    # List of datasets to process
    datasets = ['1D-exp-samp.txt', '1D-exp-uni.txt', '1D-quad-uni.txt', '1D-quad-uni-noise.txt']
    
    # Loop over each dataset
    for dataset in datasets:
        # Load dataset
        train_X, train_Y = load_data(dataset)

        # Loop over each K value (small, medium, large)
        for size, K in k_values.items():
            print(f"Processing {dataset} with K={K} ({size})")
            
            # Fit model using Random Fourier Features
            Theta, Omega, B = random_fourier_features(train_X, train_Y, num_fourier_features=K)
            
            # Visualize the fitted model
            title = f"RFF Model for {dataset} with K={K} ({size} K)"
            vis_rff_model(train_X, train_Y, Theta, Omega, B)
